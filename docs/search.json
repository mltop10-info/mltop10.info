[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OWASP Machine Learning Security Top 10 (2023)",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "index.html#project-communication",
    "href": "index.html#project-communication",
    "title": "OWASP Machine Learning Security Top 10 (2023)",
    "section": "Project Communication",
    "text": "Project Communication\n\nOWASP Slack #project-mlsec-top-10\nGithub Discussions\nMeeting Notes"
  },
  {
    "objectID": "index.html#top-10-machine-learning-security-risks",
    "href": "index.html#top-10-machine-learning-security-risks",
    "title": "OWASP Machine Learning Security Top 10 (2023)",
    "section": "Top 10 Machine Learning Security Risks",
    "text": "Top 10 Machine Learning Security Risks\n\nML01:2023 Adversarial Attack\nML02:2023 Data Poisoning Attack\nML03:2023 Model Inversion Attack\nML04:2023 Membership Inference Attack\nML05:2023 Model Stealing\nML06:2023 Corrupted Packages\nML07:2023 Transfer Learning Attack\nML08:2023 Model Skewing\nML09:2023 Output Integrity Attack\nML10:2023 Neural Net Reprogramming"
  },
  {
    "objectID": "notice.html#release",
    "href": "notice.html#release",
    "title": "Notice",
    "section": "Release",
    "text": "Release\nThis document is currently at v0.2 draft release."
  },
  {
    "objectID": "notice.html#lead-authors",
    "href": "notice.html#lead-authors",
    "title": "Notice",
    "section": "Lead Authors",
    "text": "Lead Authors\n\nShain Singh\nSagar Bhure\nRob van der Veer"
  },
  {
    "objectID": "notice.html#contributors",
    "href": "notice.html#contributors",
    "title": "Notice",
    "section": "Contributors",
    "text": "Contributors\nThanks goes to these wonderful people (emoji key):\n\n\n\n\n\n\n\n\n\nSagar Bhureüíª üìñ üëÄ üí¨ üñã üî¨ üì£\n\n\nShain Singhüíª üìñ üëÄ üí¨ üñã üì£ üìÜ\n\n\nRob van der VeerüëÄ üíª üìñ üí¨ üì£\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM S Nishanthüíª üí¨\n\n\nRick Müíª\n\n\nHarold Blankenshipüíª\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRiccardoBiosasüíª\n\n\nAryan Kenchappagolüìñ\n\n\nMiko≈Çaj Kowalczyküíª üìñ üí¨ üì£"
  },
  {
    "objectID": "notice.html#how-to-contribute",
    "href": "notice.html#how-to-contribute",
    "title": "Notice",
    "section": "How to contribute",
    "text": "How to contribute\nThis project follows the all-contributors specification. Contributions of any kind welcome!"
  },
  {
    "objectID": "ML01_2023-Adversarial_Attack.html#description",
    "href": "ML01_2023-Adversarial_Attack.html#description",
    "title": "ML01:2023 Adversarial Attack",
    "section": "Description",
    "text": "Description\nAdversarial attacks are a type of attack in which an attacker deliberately alters input data to mislead the model."
  },
  {
    "objectID": "ML01_2023-Adversarial_Attack.html#how-to-prevent",
    "href": "ML01_2023-Adversarial_Attack.html#how-to-prevent",
    "title": "ML01:2023 Adversarial Attack",
    "section": "How to Prevent",
    "text": "How to Prevent\nAdversarial training: One approach to defending against adversarial attacks is to train the model on adversarial examples. This can help the model become more robust to attacks and reduce its susceptibility to being misled.\nRobust models: Another approach is to use models that are designed to be robust against adversarial attacks, such as adversarial training or models that incorporate defense mechanisms.\nInput validation: Input validation is another important defense mechanism that can be used to detect and prevent adversarial attacks. This involves checking the input data for anomalies, such as unexpected values or patterns, and rejecting inputs that are likely to be malicious."
  },
  {
    "objectID": "ML01_2023-Adversarial_Attack.html#risk-factors",
    "href": "ML01_2023-Adversarial_Attack.html#risk-factors",
    "title": "ML01:2023 Adversarial Attack",
    "section": "Risk Factors",
    "text": "Risk Factors\n\n\n\n\n\n\n\n\nThreat Agents/Attack Vectors\nSecurity Weakness\nImpact\n\n\n\n\nExploitability: 5 (Easy)  ML Application Specific: 4  ML Operations Specific: 3\nDetectability: 3 (Moderate)  The adversarial image may not be noticeable to the naked eye, making it difficult to detect the attack.\nTechnical: 5 (Difficult)  The attack requires technical knowledge of deep learning and image processing techniques.\n\n\nThreat Agent: Attacker with knowledge of deep learning and image processing techniques.  Attack Vector: Deliberately crafted adversarial image that is similar to a legitimate image.\nVulnerability in the deep learning model‚Äôs ability to classify images accurately.\nMisclassification of the image, leading to security bypass or harm to the system.\n\n\n\nIt is important to note that this chart is only a sample based on the scenario below only. The actual risk assessment will depend on the specific circumstances of each machine learning system."
  },
  {
    "objectID": "ML01_2023-Adversarial_Attack.html#example-attack-scenarios",
    "href": "ML01_2023-Adversarial_Attack.html#example-attack-scenarios",
    "title": "ML01:2023 Adversarial Attack",
    "section": "Example Attack Scenarios",
    "text": "Example Attack Scenarios\n\nScenario #1: Image classification\nA deep learning model is trained to classify images into different categories, such as dogs and cats. An attacker creates an adversarial image that is very similar to a legitimate image of a cat, but with small, carefully crafted perturbations that cause the model to misclassify it as a dog. When the model is deployed in a real-world setting, the attacker can use the adversarial image to bypass security measures or cause harm to the system.\n\n\nScenario #2: Network intrusion detection\nA deep learning model is trained to detect intrusions in a network. An attacker creates adversarial network traffic by carefully crafting packets in such a way that they will evade the model's intrusion detection system. The attacker can manipulate the features of the network traffic, such as the source IP address, destination IP address, or payload, in such a way that they are not detected by the intrusion detection system. For example, the attacker may hide their source IP address behind a proxy server or encrypt the payload of their network traffic. This type of attack can have serious consequences, as it can lead to data theft, system compromise, or other forms of damage."
  },
  {
    "objectID": "ML01_2023-Adversarial_Attack.html#references",
    "href": "ML01_2023-Adversarial_Attack.html#references",
    "title": "ML01:2023 Adversarial Attack",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "ML02_2023-Data_Poisoning_Attack.html#description",
    "href": "ML02_2023-Data_Poisoning_Attack.html#description",
    "title": "ML02:2023 Data Poisoning Attack",
    "section": "Description",
    "text": "Description\nData poisoning attacks occur when an attacker manipulates the training data to cause the model to behave in an undesirable way."
  },
  {
    "objectID": "ML02_2023-Data_Poisoning_Attack.html#how-to-prevent",
    "href": "ML02_2023-Data_Poisoning_Attack.html#how-to-prevent",
    "title": "ML02:2023 Data Poisoning Attack",
    "section": "How to Prevent",
    "text": "How to Prevent\nData validation and verification: Ensure that the training data is thoroughly validated and verified before it is used to train the model. This can be done by implementing data validation checks and employing multiple data labelers to validate the accuracy of the data labeling.\nSecure data storage: Store the training data in a secure manner, such as using encryption, secure data transfer protocols, and firewalls.\nData separation: Separate the training data from the production data to reduce the risk of compromising the training data.\nAccess control: Implement access controls to limit who can access the training data and when they can access it.\nMonitoring and auditing: Regularly monitor the training data for any anomalies and conduct audits to detect any data tampering.\nModel validation: Validate the model using a separate validation set that has not been used during training. This can help to detect any data poisoning attacks that may have affected the training data.\nModel ensembles: Train multiple models using different subsets of the training data and use an ensemble of these models to make predictions. This can reduce the impact of data poisoning attacks as the attacker would need to compromise multiple models to achieve their goals.\nAnomaly detection: Use anomaly detection techniques to detect any abnormal behavior in the training data, such as sudden changes in the data distribution or data labeling. These techniques can be used to detect data poisoning attacks early on."
  },
  {
    "objectID": "ML02_2023-Data_Poisoning_Attack.html#risk-factors",
    "href": "ML02_2023-Data_Poisoning_Attack.html#risk-factors",
    "title": "ML02:2023 Data Poisoning Attack",
    "section": "Risk Factors",
    "text": "Risk Factors\n\n\n\n\n\n\n\n\nThreat Agents/Attack Vectors\nSecurity Weakness\nImpact\n\n\n\n\nExploitability: 3 (Moderate)  ML Application Specific: 4  ML Operations Specific: 3\nDetectability: 2 (Difficult)\nTechnical: 4 (Moderate)\n\n\nThreat Agent: Attacker who has access to the training data used for the model.  Attack Vector: The attacker injects malicious data into the training data set.\nLack of data validation and insufficient monitoring of the training data.\nThe model will make incorrect predictions based on the poisoned data, leading to false decisions and potentially serious consequences.\n\n\n\nIt is important to note that this chart is only a sample based on the scenario below only. The actual risk assessment will depend on the specific circumstances of each machine learning system."
  },
  {
    "objectID": "ML02_2023-Data_Poisoning_Attack.html#example-attack-scenarios",
    "href": "ML02_2023-Data_Poisoning_Attack.html#example-attack-scenarios",
    "title": "ML02:2023 Data Poisoning Attack",
    "section": "Example Attack Scenarios",
    "text": "Example Attack Scenarios\n\nScenario #1: Training a spam classifier\nAn attacker poisons the training data for a deep learning model that classifies emails as spam or not spam. The attacker executed this attack by injecting the maliciously labeled spam emails into the training data set. This could be done by compromising the data storage system, for example by hacking into the network or exploiting a vulnerability in the data storage software. The attacker could also manipulate the data labeling process, such as by falsifying the labeling of the emails or by bribing the data labelers to provide incorrect labels.\n\n\nScenario #2: Training a network traffic classification system\nAn attacker poisons the training data for a deep learning model that is used to classify network traffic into different categories, such as email, web browsing, and video streaming. They introduce a large number of examples of network traffic that are incorrectly labeled as a different type of traffic, causing the model to be trained to classify this traffic as the incorrect category. As a result, the model may be trained to make incorrect traffic classifications when the model is deployed, potentially leading to misallocation of network resources or degradation of network performance."
  },
  {
    "objectID": "ML02_2023-Data_Poisoning_Attack.html#references",
    "href": "ML02_2023-Data_Poisoning_Attack.html#references",
    "title": "ML02:2023 Data Poisoning Attack",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "ML03_2023-Model_Inversion_Attack.html#description",
    "href": "ML03_2023-Model_Inversion_Attack.html#description",
    "title": "ML03:2023 Model Inversion Attack",
    "section": "Description",
    "text": "Description\nModel inversion attacks occur when an attacker reverse-engineers the model to extract information from it."
  },
  {
    "objectID": "ML03_2023-Model_Inversion_Attack.html#how-to-prevent",
    "href": "ML03_2023-Model_Inversion_Attack.html#how-to-prevent",
    "title": "ML03:2023 Model Inversion Attack",
    "section": "How to Prevent",
    "text": "How to Prevent\nAccess control: Limiting access to the model or its predictions can prevent attackers from obtaining the information needed to invert the model. This can be done by requiring authentication, encryption, or other forms of security when accessing the model or its predictions.\nInput validation: Validating the inputs to the model can prevent attackers from providing malicious data that can be used to invert the model. This can be done by checking the format, range, and consistency of the inputs before they are processed by the model.\nModel transparency: Making the model and its predictions transparent can help to detect and prevent model inversion attacks. This can be done by logging all inputs and outputs, providing explanations for the model‚Äôs predictions, or allowing users to inspect the model‚Äôs internal representations.\nRegular monitoring: Monitoring the model‚Äôs predictions for anomalies can help to detect and prevent model inversion attacks. This can be done by tracking the distribution of inputs and outputs, comparing the model‚Äôs predictions to ground truth data, or monitoring the model‚Äôs performance over time.\nModel retraining: Regularly retraining the model can help to prevent the information leaked by model inversion attacks from becoming outdated. This can be done by incorporating new data and correcting any inaccuracies in the model‚Äôs predictions."
  },
  {
    "objectID": "ML03_2023-Model_Inversion_Attack.html#risk-factors",
    "href": "ML03_2023-Model_Inversion_Attack.html#risk-factors",
    "title": "ML03:2023 Model Inversion Attack",
    "section": "Risk Factors",
    "text": "Risk Factors\n\n\n\n\n\n\n\n\nThreat Agents/Attack Vectors\nSecurity Weakness\nImpact\n\n\n\n\nExploitability: 4 (Moderate)  ML Application Specific: 5  ML Operations Specific: 3\nDetectability: 2 (Difficult)\nTechnical: 4 (Moderate)\n\n\nThreat Agents: Attackers who have access to the model and input data.  Attack Vectors: Submitting an image to the model and analyzing the model‚Äôs response.\nModel‚Äôs output can be used to infer sensitive information about the input data.\nConfidential information about the input data can be compromised.\n\n\n\nIt is important to note that this chart is only a sample based on the scenario below only. The actual risk assessment will depend on the specific circumstances of each machine learning system."
  },
  {
    "objectID": "ML03_2023-Model_Inversion_Attack.html#example-attack-scenarios",
    "href": "ML03_2023-Model_Inversion_Attack.html#example-attack-scenarios",
    "title": "ML03:2023 Model Inversion Attack",
    "section": "Example Attack Scenarios",
    "text": "Example Attack Scenarios\n\nScenario #1: Stealing personal information from a face recognition model\nAn attacker trains a deep learning model to perform face recognition. They then use this model to perform a model inversion attack on a different face recognition model that is used by a company or organization. The attacker inputs images of individuals into the model and recovers the personal information of the individuals from the model's predictions, such as their name, address, or social security number.\nThe attacker executed this attack by training the model to perform face recognition and then using this model to invert the predictions of another face recognition model. This could be done by exploiting a vulnerability in the model's implementation or by accessing the model through an API. The attacker would then be able to recover the personal information of the individuals from the model's predictions.\n\n\nScenario #2: Bypassing a bot detection model in online advertising\nAn advertiser wants to automate their advertising campaigns by using bots to perform actions such as clicking on ads and visiting websites. However, online advertising platforms use bot detection models to prevent bots from performing these actions. To bypass these models, the advertiser trains a deep learning model for bot detection and uses it to invert the predictions of the bot detection model used by the online advertising platform. The advertiser inputs their bots into the model and is able to make the bots appear as human users, allowing them to bypass the bot detection and successfully execute their automated advertising campaigns.\nThe advertiser executed this attack by training their own bot detection model and then using it to reverse the predictions of the bot detection model used by the online advertising platform. They were able to access this other model through a vulnerability in its implementation or by using an API. The end result of the attack was the advertiser successfully automating their advertising campaigns by making their bots appear as human users."
  },
  {
    "objectID": "ML03_2023-Model_Inversion_Attack.html#references",
    "href": "ML03_2023-Model_Inversion_Attack.html#references",
    "title": "ML03:2023 Model Inversion Attack",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "ML04_2023-Membership_Inference_Attack.html#description",
    "href": "ML04_2023-Membership_Inference_Attack.html#description",
    "title": "ML04:2023 Membership Inference Attack",
    "section": "Description",
    "text": "Description\nMembership inference attacks occur when an attacker manipulates the model‚Äôs training data in order to cause it to behave in a way that exposes sensitive information."
  },
  {
    "objectID": "ML04_2023-Membership_Inference_Attack.html#how-to-prevent",
    "href": "ML04_2023-Membership_Inference_Attack.html#how-to-prevent",
    "title": "ML04:2023 Membership Inference Attack",
    "section": "How to Prevent",
    "text": "How to Prevent\nModel training on randomized or shuffled data: Training machine learning models on randomized or shuffled data can make it more difficult for an attacker to determine whether a particular example was included in the training dataset.\nModel Obfuscation: Obfuscating the model‚Äôs predictions by adding random noise or using differential privacy techniques can help prevent membership inference attacks by making it harder for an attacker to determine the model‚Äôs training data.\nRegularisation: Regularisation techniques such as L1 or L2 regularization can help prevent overfitting of the model to the training data, which can reduce the model‚Äôs ability to accurately determine whether a particular example was included in the training dataset.\nReducing the training data: Reducing the size of the training dataset or removing redundant or highly correlated features can help reduce the information an attacker can gain from a membership inference attack.\nTesting and monitoring: Regularly testing and monitoring the model‚Äôs behavior for anomalies can help detect and prevent membership inference attacks by detecting when an attacker is attempting to gain access to sensitive information."
  },
  {
    "objectID": "ML04_2023-Membership_Inference_Attack.html#risk-factors",
    "href": "ML04_2023-Membership_Inference_Attack.html#risk-factors",
    "title": "ML04:2023 Membership Inference Attack",
    "section": "Risk Factors",
    "text": "Risk Factors\n\n\n\n\n\n\n\n\nThreat Agents/Attack Vectors\nSecurity Weakness\nImpact\n\n\n\n\nExploitability: 4 (Moderate)  ML Application Specific: 5  ML Operations Specific: 3\nDetectability: 3 (Moderate)\nTechnical: 4 (Moderate)\n\n\nThreat Actors: Hackers or malicious actors who have access to the data and the model.  Insiders who have malicious intent or are bribed to interfere with the data.  Attact Vectors: Unsecured data transmission channels that allow unauthorized access to the data.\nLack of proper data access controls.  Lack of proper data validation and sanitization techniques.  Lack of proper data encryption.  Lack of proper data backup and recovery techniques.\nUnreliable or incorrect model predictions.  Loss of confidentiality and privacy of sensitive data.  Legal and regulatory compliance violations.  Reputational damage.\n\n\n\nIt is important to note that this chart is only a sample based on the scenario below only. The actual risk assessment will depend on the specific circumstances of each machine learning system."
  },
  {
    "objectID": "ML04_2023-Membership_Inference_Attack.html#example-attack-scenarios",
    "href": "ML04_2023-Membership_Inference_Attack.html#example-attack-scenarios",
    "title": "ML04:2023 Membership Inference Attack",
    "section": "Example Attack Scenarios",
    "text": "Example Attack Scenarios\n\nScenario #1: Inferencing financial data from a machine learning model\nA malicious attacker wants to gain access to sensitive financial information of individuals. They do this by training a machine learning model on a dataset of financial records and using it to query whether or not a particular individual‚Äôs record was included in the training data. The attacker can then use this information to infer the financial history and sensitive information of individuals.\nThe attacker executed this attack by training a machine learning model on a dataset of financial records obtained from a financial organization. They then used this model to query whether or not a particular individual's record was included in the training data, allowing them to infer sensitive financial information.\nReferences:"
  },
  {
    "objectID": "ML05_2023-Model_Stealing.html#description",
    "href": "ML05_2023-Model_Stealing.html#description",
    "title": "ML05:2023 Model Stealing",
    "section": "Description",
    "text": "Description\nModel stealing attacks occur when an attacker gains access to the model‚Äôs parameters."
  },
  {
    "objectID": "ML05_2023-Model_Stealing.html#how-to-prevent",
    "href": "ML05_2023-Model_Stealing.html#how-to-prevent",
    "title": "ML05:2023 Model Stealing",
    "section": "How to Prevent",
    "text": "How to Prevent\nEncryption: Encrypting the model‚Äôs code, training data, and other sensitive information can prevent attackers from being able to access and steal the model.\nAccess Control: Implementing strict access control measures, such as two-factor authentication, can prevent unauthorized individuals from accessing and stealing the model.\nRegular backups: Regularly backing up the model‚Äôs code, training data, and other sensitive information can ensure that it can be recovered in the event of a theft.\nModel Obfuscation: Obfuscating the model‚Äôs code and making it difficult to reverse engineer can prevent attackers from being able to steal the model.\nWatermarking: Adding a watermark to the model‚Äôs code and training data can make it possible to trace the source of a theft and hold the attacker accountable.\nLegal protection: Securing legal protection for the model, such as patents or trade secrets, can make it more difficult for an attacker to steal the model and can provide a basis for legal action in the event of a theft.\nMonitoring and auditing: Regularly monitoring and auditing the model‚Äôs use can help detect and prevent theft by detecting when an attacker is attempting to access or steal the model."
  },
  {
    "objectID": "ML05_2023-Model_Stealing.html#risk-factors",
    "href": "ML05_2023-Model_Stealing.html#risk-factors",
    "title": "ML05:2023 Model Stealing",
    "section": "Risk Factors",
    "text": "Risk Factors\n\n\n\n\n\n\n\n\nThreat Agents/Attack Vectors\nSecurity Weakness\nImpact\n\n\n\n\nExploitability: 4 (Moderate)  ML Application Specific: 4  ML Operations Specific: 3\nDetectability: 3 (Moderate)\nTechnical: 4 (Moderate)\n\n\nThreat Agent: This refers to the entity that carries out the attack, in this case, it is an attacker who wants to steal the machine learning model.\nUnsecured model deployment: The unsecured deployment of the model makes it easier for the attacker to access and steal the model.\nThe impact of a model theft could be both on the confidentiality of the data used to train the model and the reputation of the organization that developed the model.\n\n\n\nIt is important to note that this chart is only a sample based on the scenario below only. The actual risk assessment will depend on the specific circumstances of each machine learning system."
  },
  {
    "objectID": "ML05_2023-Model_Stealing.html#example-attack-scenarios",
    "href": "ML05_2023-Model_Stealing.html#example-attack-scenarios",
    "title": "ML05:2023 Model Stealing",
    "section": "Example Attack Scenarios",
    "text": "Example Attack Scenarios\n\nScenario #1: Stealing a machine learning model from a competitor\nA malicious attacker is working for a competitor of a company that has developed a valuable machine learning model. The attacker wants to steal this model so that their company can gain a competitive advantage and start using it for their own purposes.\nThe attacker executed this attack by reverse engineering the company‚Äôs machine learning model, either by disassembling the binary code or by accessing the model‚Äôs training data and algorithm. Once the attacker has reverse engineered the model, they can use this information to recreate the model and start using it for their own purposes. This can result in significant financial loss for the original company, as well as damage to their reputation."
  },
  {
    "objectID": "ML05_2023-Model_Stealing.html#references",
    "href": "ML05_2023-Model_Stealing.html#references",
    "title": "ML05:2023 Model Stealing",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "ML06_2023-Corrupted_Packages.html#description",
    "href": "ML06_2023-Corrupted_Packages.html#description",
    "title": "ML06:2023 Corrupted Packages",
    "section": "Description",
    "text": "Description\nCorrupted packages attacks occur when an attacker modifies or replaces a machine learning library or model that is used by a system."
  },
  {
    "objectID": "ML06_2023-Corrupted_Packages.html#how-to-prevent",
    "href": "ML06_2023-Corrupted_Packages.html#how-to-prevent",
    "title": "ML06:2023 Corrupted Packages",
    "section": "How to Prevent",
    "text": "How to Prevent\nVerify Package Signatures: Before installing any packages, verify the digital signatures of the packages to ensure that they have not been tampered with.\nUse Secure Package Repositories: Use secure package repositories, such as Anaconda, that enforce strict security measures and have a vetting process for packages.\nKeep Packages Up-to-date: Regularly update all packages to ensure that any vulnerabilities are patched.\nUse Virtual Environments: Use virtual environments to isolate packages and libraries from the rest of the system. This makes it easier to detect any malicious packages and remove them.\nPerform Code Reviews: Regularly perform code reviews on all packages and libraries used in a project to detect any malicious code.\nUse Package Verification Tools: Use tools such as PEP 476 and Secure Package Install to verify the authenticity and integrity of packages before installation.\nEducate Developers: Educate developers on the risks associated with Corrupted Packages Attacks and the importance of verifying packages before installation."
  },
  {
    "objectID": "ML06_2023-Corrupted_Packages.html#risk-factors",
    "href": "ML06_2023-Corrupted_Packages.html#risk-factors",
    "title": "ML06:2023 Corrupted Packages",
    "section": "Risk Factors",
    "text": "Risk Factors\n\n\n\n\n\n\n\n\nThreat Agents/Attack Vectors\nSecurity Weakness\nImpact\n\n\n\n\nExploitability: 5 (Easy)  ML Application Specific: 5  ML Operations Specific: 3\nDetectability: 5 (Easy)\nTechnical: 4 (Moderate)\n\n\nThreat Actor: Malicious attacker.  Attack Vector: Modifying code of open-source package used by the machine learning project.\nRelying on untrusted third-party code.\nCompromise of the machine learning project and potential harm to the organization.\n\n\n\nIt is important to note that this chart is only a sample based on the scenario below only. The actual risk assessment will depend on the specific circumstances of each machine learning system."
  },
  {
    "objectID": "ML06_2023-Corrupted_Packages.html#example-attack-scenarios",
    "href": "ML06_2023-Corrupted_Packages.html#example-attack-scenarios",
    "title": "ML06:2023 Corrupted Packages",
    "section": "Example Attack Scenarios",
    "text": "Example Attack Scenarios\n\nScenario #1: Attack on a machine learning project in an organization\nA malicious attacker wants to compromise a machine learning project being developed by a large organization. The attacker knows that the project relies on several open-source packages and libraries and wants to find a way to compromise the project.\nThe attacker executed the attack by modifying the code of one of the packages that the project relies on, such as NumPy or Scikit-learn. The attacker then uploads this modified version of the package to a public repository, such as PyPI, making it available for others to download and use. When the victim organization downloads and installs the package, the attacker‚Äôs malicious code is also installed and can be used to compromise the project.\nThis type of attack can be particularly dangerous as it can go unnoticed for a long time, since the victim may not realize that the package they are using has been compromised. The attacker‚Äôs malicious code could be used to steal sensitive information, modify results, or even cause the machine learning model to fail."
  },
  {
    "objectID": "ML06_2023-Corrupted_Packages.html#references",
    "href": "ML06_2023-Corrupted_Packages.html#references",
    "title": "ML06:2023 Corrupted Packages",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "ML07_2023-Transfer_Learning_Attack.html#description",
    "href": "ML07_2023-Transfer_Learning_Attack.html#description",
    "title": "ML07:2023 Transfer Learning Attack",
    "section": "Description",
    "text": "Description\nTransfer learning attacks occur when an attacker trains a model on one task and then fine-tunes it on another task to cause it to behave in an undesirable way."
  },
  {
    "objectID": "ML07_2023-Transfer_Learning_Attack.html#how-to-prevent",
    "href": "ML07_2023-Transfer_Learning_Attack.html#how-to-prevent",
    "title": "ML07:2023 Transfer Learning Attack",
    "section": "How to Prevent",
    "text": "How to Prevent\nRegularly monitor and update the training datasets: Regularly monitoring and updating the training datasets can help prevent the transfer of malicious knowledge from the attacker's model to the target model.\nUse secure and trusted training datasets: Using secure and trusted training datasets can help prevent the transfer of malicious knowledge from the attacker‚Äôs model to the target model.\nImplement model isolation: Implementing model isolation can help prevent the transfer of malicious knowledge from one model to another. For example, separating the training and deployment environments can prevent attackers from transferring knowledge from the training environment to the deployment environment.\nUse differential privacy: Using differential privacy can help protect the privacy of individual records in the training dataset and prevent the transfer of malicious knowledge from the attacker‚Äôs model to the target model.\nPerform regular security audits: Regular security audits can help identify and prevent transfer learning attacks by identifying and addressing vulnerabilities in the system."
  },
  {
    "objectID": "ML07_2023-Transfer_Learning_Attack.html#risk-factors",
    "href": "ML07_2023-Transfer_Learning_Attack.html#risk-factors",
    "title": "ML07:2023 Transfer Learning Attack",
    "section": "Risk Factors",
    "text": "Risk Factors\n\n\n\n\n\n\n\n\nThreat Agents/Attack Vectors\nSecurity Weakness\nImpact\n\n\n\n\nExploitability: 5 (Easy)  ML Application Specific: 4 The attack specifically targets the machine learning application and can cause significant harm to the model and the organization.  ML Operations Specific: 3  The attack requires knowledge of machine learning operations but can be executed with relative ease.\nDetectability: 1 (Difficult)  The attack may be difficult to detect as the results produced by the compromised model may appear to be correct and consistent with expectations.\nTechnical: 5 (Difficult)  The attack requires a high level of technical expertise in machine learning and a willingness to compromise the integrity of the training dataset or pre-trained models.\n\n\nThreat Actor: Malicious actor.  Attack Vector: Attacker with knowledge of machine learning and access to the training dataset or pre-trained models.\nLack of proper data protection measures for the training dataset and pre-trained models.  Insecure storage and sharing of pre-trained models.  Lack of proper data protection measures for the pre-trained models and training dataset.\nMisleading or incorrect results from the machine learning model.  Confidentiality breach of sensitive information in the training dataset.  Reputational harm to the organization.  Legal or regulatory compliance issues.\n\n\n\nIt is important to note that this chart is only a sample based on the scenario below only. The actual risk assessment will depend on the specific circumstances of each machine learning system."
  },
  {
    "objectID": "ML07_2023-Transfer_Learning_Attack.html#example-attack-scenarios",
    "href": "ML07_2023-Transfer_Learning_Attack.html#example-attack-scenarios",
    "title": "ML07:2023 Transfer Learning Attack",
    "section": "Example Attack Scenarios",
    "text": "Example Attack Scenarios\n\nScenario #1: Training a model on a malicious dataset\nAn attacker trains a machine learning model on a malicious dataset that contains manipulated images of faces. The attacker wants to target a face recognition system used by a security firm for identity verification.\nThe attacker then transfers the model‚Äôs knowledge to the target face recognition system. The target system starts using the attacker‚Äôs manipulated model for identity verification.\nAs a result, the face recognition system starts making incorrect predictions, allowing the attacker to bypass the security and gain access to sensitive information. For example, the attacker could use a manipulated image of themselves and the system would identify them as a legitimate user."
  },
  {
    "objectID": "ML07_2023-Transfer_Learning_Attack.html#references",
    "href": "ML07_2023-Transfer_Learning_Attack.html#references",
    "title": "ML07:2023 Transfer Learning Attack",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "ML08_2023-Model_Skewing.html#description",
    "href": "ML08_2023-Model_Skewing.html#description",
    "title": "ML08:2023 Model Skewing",
    "section": "Description",
    "text": "Description\nModel skewing attacks occur when an attacker manipulates the distribution of the training data to cause the model to behave in an undesirable way."
  },
  {
    "objectID": "ML08_2023-Model_Skewing.html#how-to-prevent",
    "href": "ML08_2023-Model_Skewing.html#how-to-prevent",
    "title": "ML08:2023 Model Skewing",
    "section": "How to Prevent",
    "text": "How to Prevent\nImplement robust access controls: Ensure that only authorized personnel have access to the MLOps system and its feedback loops, and that all activities are logged and audited.\nVerify the authenticity of feedback data: Use techniques such as digital signatures and checksums to verify that the feedback data received by the system is genuine, and reject any data that does not match the expected format.\nUse data validation and cleaning techniques: Clean and validate the feedback data before using it to update the training data, to minimize the risk of incorrect or malicious data being used.\nImplement anomaly detection: Use techniques such as statistical and machine learning-based methods to detect and alert on anomalies in the feedback data, which could indicate an attack.\nRegularly monitor the model‚Äôs performance: Continuously monitor the performance of the model, and compare its predictions with actual outcomes to detect any deviation or skewing.\nContinuously train the model: Regularly retrain the model using updated and verified training data, to ensure that it continues to reflect the latest information and trends."
  },
  {
    "objectID": "ML08_2023-Model_Skewing.html#risk-factors",
    "href": "ML08_2023-Model_Skewing.html#risk-factors",
    "title": "ML08:2023 Model Skewing",
    "section": "Risk Factors",
    "text": "Risk Factors\n\n\n\n\n\n\n\n\nThreat Agents/Attack Vectors\nSecurity Weakness\nImpact\n\n\n\n\nExploitability: 5 (Easy)  ML Application Specific: 4  The attacker has a clear understanding of the machine learning project and its vulnerabilities.  ML Operations Specific: 3  Manipulation of the training data requires knowledge of the machine learning process.\nDetectability: 2 (Difficult)  The model skewing might not be easily noticeable during the testing phase.\nTechnical: 5 (Difficult)  Manipulation of the training data is a technically complex task.\n\n\nThreat Actors: Malicious actors or a third-party with a vested interest in manipulating the outcomes of a model.\nInability of the model to accurately reflect the underlying distribution of the training data.  This can occur due to factors such as data bias, incorrect sampling of the data, or manipulation of the data or training process by an attacker.\nSignificant risk which can lead to incorrect decisions being made based on the output of the model.  This can result in financial loss, damage to reputation, and even harm to individuals if the model is being used for critical applications such as medical diagnosis or criminal justice.\n\n\n\nIt is important to note that this chart is only a sample based on the scenario below only. The actual risk assessment will depend on the specific circumstances of each machine learning system."
  },
  {
    "objectID": "ML08_2023-Model_Skewing.html#example-attack-scenarios",
    "href": "ML08_2023-Model_Skewing.html#example-attack-scenarios",
    "title": "ML08:2023 Model Skewing",
    "section": "Example Attack Scenarios",
    "text": "Example Attack Scenarios\n\nScenario #1: Financial gain through model skewing\nA financial institution is using a machine learning model to predict the creditworthiness of loan applicants, and the model‚Äôs predictions are integrated into their loan approval process. An attacker wants to increase their chances of getting a loan approved, so they manipulate the feedback loop in the MLOps system. The attacker provides fake feedback data to the system, indicating that high-risk applicants have been approved for loans in the past, and this feedback is used to update the model‚Äôs training data. As a result, the model‚Äôs predictions are skewed towards low-risk applicants, and the attacker‚Äôs chances of getting a loan approved are significantly increased.\nThis type of attack can compromise the accuracy and fairness of the model, leading to unintended consequences and potential harm to the financial institution and its customers."
  },
  {
    "objectID": "ML08_2023-Model_Skewing.html#references",
    "href": "ML08_2023-Model_Skewing.html#references",
    "title": "ML08:2023 Model Skewing",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "ML09_2023-Output_Integrity_Attack.html#description",
    "href": "ML09_2023-Output_Integrity_Attack.html#description",
    "title": "ML09:2023 Output Integrity Attack",
    "section": "Description",
    "text": "Description\nIn an Output Integrity Attack scenario, an attacker aims to modify or manipulate the output of a machine learning model in order to change its behavior or cause harm to the system it is used in."
  },
  {
    "objectID": "ML09_2023-Output_Integrity_Attack.html#how-to-prevent",
    "href": "ML09_2023-Output_Integrity_Attack.html#how-to-prevent",
    "title": "ML09:2023 Output Integrity Attack",
    "section": "How to Prevent",
    "text": "How to Prevent\nUsing cryptographic methods: Cryptographic methods like digital signatures and secure hashes can be used to verify the authenticity of the results.\nSecure communication channels: Communication channels between the model and the interface responsible for displaying the results should be secured using secure protocols such as SSL/TLS.\nInput Validation: Input validation should be performed on the results to check for unexpected or manipulated values.\nTamper-evident logs: Maintaining tamper-evident logs of all input and output interactions can help detect and respond to any output integrity attacks.\nRegular software updates: Regular software updates to fix vulnerabilities and security patches can help reduce the risk of output integrity attacks.\nMonitoring and auditing: Regular monitoring and auditing of the results and the interactions between the model and the interface can help detect any suspicious activities and respond accordingly."
  },
  {
    "objectID": "ML09_2023-Output_Integrity_Attack.html#risk-factors",
    "href": "ML09_2023-Output_Integrity_Attack.html#risk-factors",
    "title": "ML09:2023 Output Integrity Attack",
    "section": "Risk Factors",
    "text": "Risk Factors\n\n\n\n\n\n\n\n\nThreat Agents/Attack Vectors\nSecurity Weakness\nImpact\n\n\n\n\nExploitability: 5 (Easy)  ML Application Specific: 4  ML Operations Specific: 4\nDetectability: 3 (Moderate)\nTechnical: 3 (Moderate)\n\n\nThreat Actors: Malicious attackers or insiders who have access to the model‚Äôs inputs and outputs.  Third-party entities who have access to the inputs and outputs and may tamper with them to achieve a certain outcome.\nLack of proper authentication and authorization measures to ensure the integrity of the inputs and outputs.  Inadequate validation and verification of inputs and outputs to prevent tampering.  Insufficient monitoring and logging of inputs and outputs to detect tampering.\nLoss of confidence in the model‚Äôs predictions and results.  Financial loss or damage to reputation if the model‚Äôs predictions are used to make important decisions.  Security risks if the model is used in a critical application such as financial fraud detection or cybersecurity.\n\n\n\nIt is important to note that this chart is only a sample based on the scenario below only. The actual risk assessment will depend on the specific circumstances of each machine learning system."
  },
  {
    "objectID": "ML09_2023-Output_Integrity_Attack.html#example-attack-scenarios",
    "href": "ML09_2023-Output_Integrity_Attack.html#example-attack-scenarios",
    "title": "ML09:2023 Output Integrity Attack",
    "section": "Example Attack Scenarios",
    "text": "Example Attack Scenarios\n\nScenario #1: Modification of patient health records\nAn attacker has gained access to the output of a machine learning model that is being used to diagnose diseases in a hospital. The attacker modifies the output of the model, making it provide incorrect diagnoses for patients. As a result, patients are given incorrect treatments, leading to further harm and potentially even death."
  },
  {
    "objectID": "ML09_2023-Output_Integrity_Attack.html#references",
    "href": "ML09_2023-Output_Integrity_Attack.html#references",
    "title": "ML09:2023 Output Integrity Attack",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "ML10_2023-Neural_Net_Reprogramming.html#description",
    "href": "ML10_2023-Neural_Net_Reprogramming.html#description",
    "title": "ML10:2023 Neural Net Reprogramming",
    "section": "Description",
    "text": "Description\nNeural net reprogramming attacks occur when an attacker manipulates the model's parameters to cause it to behave in an undesirable way."
  },
  {
    "objectID": "ML10_2023-Neural_Net_Reprogramming.html#how-to-prevent",
    "href": "ML10_2023-Neural_Net_Reprogramming.html#how-to-prevent",
    "title": "ML10:2023 Neural Net Reprogramming",
    "section": "How to Prevent",
    "text": "How to Prevent\nRegularisation: Adding regularisation techniques like L1 or L2 regularization to the loss function helps to prevent overfitting and reduce the chance of neural net reprogramming attacks.\nRobust Model Design: Designing models with robust architectures and activation functions can help reduce the chances of successful reprogramming attacks.\nCryptographic Techniques: Cryptographic techniques can be used to secure the parameters and weights of the model, and prevent unauthorized access or manipulation of these parameters."
  },
  {
    "objectID": "ML10_2023-Neural_Net_Reprogramming.html#risk-factors",
    "href": "ML10_2023-Neural_Net_Reprogramming.html#risk-factors",
    "title": "ML10:2023 Neural Net Reprogramming",
    "section": "Risk Factors",
    "text": "Risk Factors\n\n\n\n\n\n\n\n\nThreat Agents/Attack Vectors\nSecurity Weakness\nImpact\n\n\n\n\nExploitability: 5 (Easy)  ML Application Specific: 4  ML Operations Specific: 4\nDetectability: 3 (Moderate)\nTechnical: 3 (Moderate)\n\n\nThreat Actor: Malicious individuals or organizations with knowledge and resources to manipulate deep learning models.  Malicious insiders within the organization developing the deep learning model.\nInsufficient access controls to the model‚Äôs code and parameters.  Lack of proper secure coding practices.  Inadequate monitoring and logging of model‚Äôs activity.\nModel‚Äôs predictions can be manipulated to achieve desired results. Confidential information within the model can be extracted.  Decisions based on the model‚Äôs predictions can be impacted negatively.  Reputation and credibility of the organization can be affected.\n\n\n\nIt is important to note that this chart is only a sample based on the scenario below only. The actual risk assessment will depend on the specific circumstances of each machine learning system."
  },
  {
    "objectID": "ML10_2023-Neural_Net_Reprogramming.html#example-attack-scenarios",
    "href": "ML10_2023-Neural_Net_Reprogramming.html#example-attack-scenarios",
    "title": "ML10:2023 Neural Net Reprogramming",
    "section": "Example Attack Scenarios",
    "text": "Example Attack Scenarios\n\nScenario #1: Financial gain through neural net reprogramming\nConsider a scenario where a bank is using a machine learning model to identify handwritten characters on cheques to automate their clearing process. The model has been trained on a large dataset of handwritten characters, and it has been designed to accurately identify the characters based on specific parameters such as size, shape, slant, and spacing.\nAn attacker who wants to exploit the Neural Net Reprogramming attack may manipulate the parameters of the model by altering the images in the training dataset or directly modifying the parameters in the model. This can result in the model being reprogrammed to identify characters differently. For example, the attacker could change the parameters so that the model identifies the character ‚Äú5‚Äù as the character ‚Äú2‚Äù, leading to incorrect amounts being processed.\nThe attacker can exploit this vulnerability by introducing forged cheques into the clearing process, which the model will process as valid due to the manipulated parameters. This can result in significant financial loss to the bank."
  },
  {
    "objectID": "ML10_2023-Neural_Net_Reprogramming.html#references",
    "href": "ML10_2023-Neural_Net_Reprogramming.html#references",
    "title": "ML10:2023 Neural Net Reprogramming",
    "section": "References",
    "text": "References"
  }
]